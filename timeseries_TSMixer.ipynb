{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDataLoader\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Revised base on https://zhuanlan.zhihu.com/p/667766822\n",
    "'''\n",
    "\n",
    "# 创建DataLoader进行数据集转换\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "    device = 'cuda'\n",
    "\n",
    "class DataLoader:\n",
    "\n",
    "    def __init__(self, batch_size, seq_len, pred_len):\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.target_slice = slice(0, None)\n",
    "\n",
    "        self._read_data()\n",
    "\n",
    "    # 读取数据\n",
    "    def _read_data(self):\n",
    "        '''\n",
    "        扩展数据以缩短模型的训练时间至关重要；\n",
    "        将缩放器安装在训练集上只是为了避免验证和测试集中的数据泄漏\n",
    "        '''\n",
    "\n",
    "        filepath = ('../Dataset/Simulate_Data/set_1/File_indexed.xlsx')\n",
    "\n",
    "        df_raw = pd.read_excel(filepath)\n",
    "        df = df_raw.set_index('TIME') # 以时间步为编号变量\n",
    "\n",
    "        # split train/valid/test，以0.7/0.2/0.1的比例分割原数据集\n",
    "        n = len(df)\n",
    "        train_end = int(n * 0.7)\n",
    "        val_end = n - int(n * 0.2)\n",
    "        test_end = n\n",
    "\n",
    "        train_df = df[:train_end]\n",
    "        val_df = df[train_end - self.seq_len : val_end]\n",
    "        test_df = df[val_end - self.seq_len : test_end]\n",
    "\n",
    "        # standardize by training set\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaler.fit(train_df.values)\n",
    "\n",
    "        def scale_df(df, scaler):\n",
    "            data = scaler.transform(df.values)\n",
    "            return pd.DataFrame(data, index=df.index, columns=df.columns)\n",
    "\n",
    "        self.train_df = scale_df(train_df, self.scaler)\n",
    "        self.val_df = scale_df(val_df, self.scaler)\n",
    "        self.test_df = scale_df(test_df, self.scaler)\n",
    "        self.n_feature = self.train_df.shape[-1]\n",
    "\n",
    "    # 将数据窗口分割为输入和标签，现在暂不需要\n",
    "    def _split_window(self, data):\n",
    "        inputs = data[:, : self.seq_len, :]\n",
    "        labels = data[:, self.seq_len :, self.target_slice]\n",
    "\n",
    "        inputs.set_shape([None, self.seq_len, None])\n",
    "        labels.set_shape([None, self.pred_len, None])\n",
    "        return inputs, labels\n",
    "\n",
    "    # 将数据重组为[num of samples, seq_len, input_size]的形式\n",
    "    def _make_dataset(self, data, shuffle=True):\n",
    "        data = np.array(data, dtype=np.float32) # 此时的data还是[total time length, input_size]的形式\n",
    "\n",
    "        # # tensorflow 版本\n",
    "        # ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "        #     data=data,\n",
    "        #     targets=None,\n",
    "        #     sequence_length=(self.seq_len + self.pred_len),\n",
    "        #     sequence_stride=1,\n",
    "        #     shuffle=shuffle,\n",
    "        #     batch_size=self.batch_size,\n",
    "        # )\n",
    "        # ds = ds.map(self._split_window)\n",
    "\n",
    "        # pytorch 版本\n",
    "        sample_size = data.shape[0] // (self.seq_len + self.pred_len)\n",
    "        data = data.reshape(sample_size, self.seq_len + self.pred_len, self.n_feature)\n",
    "        data_tensor = torch.tensor(data, dtype=torch.float32)\n",
    "        # 创建一个 TensorDataset\n",
    "        dataset = TensorDataset(data_tensor)\n",
    "        # 创建一个 DataLoader\n",
    "        batch_size = 32\n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "        return data_loader\n",
    "\n",
    "    # DataLoader通过对预测进行逆变换，生成训练集、验证集和测试集\n",
    "    def inverse_transform(self, data):\n",
    "        return self.scaler.inverse_transform(data)\n",
    "\n",
    "    def get_train(self, shuffle=True):\n",
    "        # 在训练集中打乱顺序进行训练\n",
    "        return self._make_dataset(self.train_df, shuffle=shuffle)\n",
    "\n",
    "    def get_val(self):\n",
    "        return self._make_dataset(self.val_df, shuffle=False)\n",
    "\n",
    "    def get_test(self):\n",
    "        return self._make_dataset(self.test_df, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建TSMixer网络, tensorflow版本\n",
    "\n",
    "# from tensorflow.keras import layers\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def res_block(inputs, norm_type, activation, dropout, ff_dim):\n",
    "    '''\n",
    "    定义残差模块\n",
    "    '''\n",
    "    # norm = layers.BatchNormalization\n",
    "\n",
    "    batch_norm = nn.BatchNorm1d(num_features)\n",
    "\n",
    "    # Time mixing\n",
    "    '''\n",
    "    混合器层，它包括：\n",
    "        批量归一化\n",
    "        转置矩阵\n",
    "        通过 ReLu 激活馈送到全连接层\n",
    "        再次转置\n",
    "        漏失层\n",
    "        并在最后添加残差\n",
    "    '''\n",
    "    x = norm(axis=[-2, -1])(inputs)\n",
    "    x = tf.transpose(x, perm=[0, 2, 1])  # [Batch, Channel, Input Length]\n",
    "    x = layers.Dense(x.shape[-1], activation='relu')(x)\n",
    "    x = tf.transpose(x, perm=[0, 2, 1])  # [Batch, Input Length, Channel]\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feature mixing\n",
    "    '''\n",
    "    特征混合部分，其中：\n",
    "        批量归一化\n",
    "        致密层\n",
    "        一个dropout层\n",
    "        另一个致密层\n",
    "        另一个dropout层\n",
    "        并添加残差以进行残差连接\n",
    "    '''\n",
    "    x = norm(axis=[-2, -1])(res)\n",
    "    x = layers.Dense(ff_dim, activation='relu')(x)  # [Batch, Input Length, FF_Dim]\n",
    "    x = layers.Dropout(0.7)(x)\n",
    "    x = layers.Dense(inputs.shape[-1])(x)  # [Batch, Input Length, Channel]\n",
    "    x = layers.Dropout(0.7)(x)\n",
    "    return x + res\n",
    "\n",
    "def build_model(\n",
    "    input_shape,\n",
    "    pred_len,\n",
    "    n_block,\n",
    "    ff_dim,\n",
    "    target_slice,\n",
    "):\n",
    "\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = inputs  # [Batch, Input Length, Channel]\n",
    "    for _ in range(n_block):\n",
    "        x = res_block(x, norm_type, activation, dropout, ff_dim)\n",
    "\n",
    "    if target_slice:\n",
    "        x = x[:, :, target_slice]\n",
    "\n",
    "  # Temporal projection\n",
    "    '''\n",
    "    时间投影步骤：\n",
    "        转置\n",
    "        穿过致密层\n",
    "        最终转置\n",
    "    '''\n",
    "    x = tf.transpose(x, perm=[0, 2, 1])  # [Batch, Channel, Input Length]\n",
    "    x = layers.Dense(pred_len)(x)  # [Batch, Channel, Output Length]\n",
    "    outputs = tf.transpose(x, perm=[0, 2, 1])  # [Batch, Output Length, Channel])\n",
    "\n",
    "    return tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建TSMixer网络, pytorch版本\n",
    "\n",
    "# from tensorflow.keras import layers\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TSMixer(nn.Module):\n",
    "    def __init__(self, input_size, seq_len, pred_len, n_block, ff_dim, dropout_proba=0.2):\n",
    "        super().__init__()\n",
    "        # 定义归一化模块\n",
    "        self.norm = nn.LayerNorm(input_size)\n",
    "        # 定义残差块数量\n",
    "        self.n_block = n_block\n",
    "        # 定义致密层\n",
    "        self.dense_in = nn.Linear(seq_len, seq_len)\n",
    "        self.dense_out = nn.Linear(seq_len, pred_len)\n",
    "        self.dense_ff_in = nn.Linear(seq_len, ff_dim)\n",
    "        self.dense_ff_out = nn.Linear(ff_dim, seq_len)\n",
    "        # 定义激活函数\n",
    "        self.activate = nn.Relu()\n",
    "        # 定义dropout模块\n",
    "        self.dropout = nn.Dropout(p=dropout_proba)\n",
    "\n",
    "    def res_block(self, inputs):\n",
    "        '''\n",
    "        定义残差模块\n",
    "        '''\n",
    "        # Time mixing\n",
    "        '''\n",
    "        混合器层，它包括：\n",
    "            批量归一化\n",
    "            转置矩阵\n",
    "            通过 ReLu 激活馈送到全连接层\n",
    "            再次转置\n",
    "            漏失层\n",
    "            并在最后添加残差\n",
    "        '''\n",
    "        x = self.norm(inputs)\n",
    "        x = x.permute(0, 2, 1)  # [Batch, Channel, Input Length]\n",
    "        x = self.dense_in(x)\n",
    "        x = self.activate(x)\n",
    "        x = x.permute(0, 2, 1)  # [Batch, Input Length, Channel]\n",
    "        x = self.dropout(x)\n",
    "        res = x + inputs\n",
    "\n",
    "        # Feature mixing\n",
    "        '''\n",
    "        特征混合部分，其中：\n",
    "            批量归一化\n",
    "            致密层\n",
    "            一个dropout层\n",
    "            另一个致密层\n",
    "            另一个dropout层\n",
    "            并添加残差以进行残差连接\n",
    "        '''\n",
    "        x = self.norm(res)\n",
    "        x = self.dense_ff_in(x)\n",
    "        x = self.activate(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense_ff_out(x)\n",
    "        x = self.dropout(x)\n",
    "        return x + res\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 输入时：[Batch, Input Length, Channel]\n",
    "        for _ in range(self.n_block):\n",
    "            x = res_block(x)\n",
    "\n",
    "        # Temporal projection\n",
    "        '''\n",
    "        时间投影步骤：\n",
    "            转置\n",
    "            穿过致密层\n",
    "            最终转置\n",
    "        '''\n",
    "        x = x.permute(0, 2, 1)  # [Batch, Channel, Input Length]\n",
    "        x = self.dense_out(x)  # [Batch, Channel, Output Length]\n",
    "        x = x.permute(0, 2, 1)  # [Batch, Output Length, Channel])\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_core.keras.preprocessing' has no attribute 'timeseries_dataset_from_array'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_43512\\3974373114.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdata_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m96\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mval_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_val\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_43512\\3428832544.py\u001b[0m in \u001b[0;36mget_train\u001b[1;34m(self, shuffle)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_val\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_43512\\3428832544.py\u001b[0m in \u001b[0;36m_make_dataset\u001b[1;34m(self, data, shuffle)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n\u001b[0m\u001b[0;32m     69\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0mtargets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow_core.keras.preprocessing' has no attribute 'timeseries_dataset_from_array'"
     ]
    }
   ],
   "source": [
    "# 初始化DataLoader来读取数据集并创建训练集、验证集和测试集\n",
    "seq_len = 8\n",
    "pre_len = 16\n",
    "data_loader = DataLoader(batch_size=32, seq_len=seq_len, pred_len=pre_len)\n",
    "\n",
    "train_data = data_loader.get_train()\n",
    "val_data = data_loader.get_val()\n",
    "test_data = data_loader.get_test()\n",
    "\n",
    "# 初始化TSMixer模型\n",
    "model = TSMixer(\n",
    "    input_size = data_loader.n_feature,\n",
    "    seq_len = seq_len,\n",
    "    pred_len=pre_len,\n",
    "    n_block=8,\n",
    "    ff_dim=64,\n",
    "    dropout_proba=0.2\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "'''\n",
    "使用学习率为 1e-4 的 Adam 优化器;\n",
    "实施检查点来保存最佳模型;\n",
    "在连续三个时期没有改进的情况下提前停止以停止训练。\n",
    "'''\n",
    "torch.manual_seed(42)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# model.compile(optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "# checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "#     filepath='tsmixer_checkpoints/',\n",
    "#     vebose=1,\n",
    "#     save_best_only=True,\n",
    "#     save_weights_only=True\n",
    "# )\n",
    "\n",
    "# early_stop_callback = tf.keras.callbacks.EarlyStopping(\n",
    "#     monitor='val_loss',\n",
    "#     patience=3\n",
    "# )\n",
    "\n",
    "# history = model.fit(\n",
    "#     train_data,\n",
    "#     epochs= 30,\n",
    "#     validation_data=val_data,\n",
    "#     callbacks=[checkpoint_callback, early_stop_callback]\n",
    "# )\n",
    "\n",
    "# 训练模型\n",
    "epochs = 50\n",
    "train_loss = []\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_data)\n",
    "    loss = criterion(output, output)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss.append(loss.item())\n",
    "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, loss.item()))\n",
    "\n",
    "# # 在模型训练完毕后加载检查点回调保存的最佳模型\n",
    "# best_epoch = np.argmin(history.history['val_loss'])\n",
    "\n",
    "# model.load_weights(\"tsmixer_checkpoints/\")\n",
    "\n",
    "# # 访问96个时间步长的最后一个窗口的预测（已按比例缩放）\n",
    "# predictions = model.predict(test_data)\n",
    "\n",
    "# scaled_preds = predictions[-1,:,:]\n",
    "\n",
    "# 将缩放和逆变换的预测存储在 DataFrame 中，以评估性能并稍后绘制预测\n",
    "cols = ['HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL', 'OT']\n",
    "\n",
    "scaled_preds_df = pd.DataFrame(scaled_preds)\n",
    "scaled_preds_df.columns = cols\n",
    "\n",
    "preds = data_loader.inverse_transform(scaled_preds)\n",
    "\n",
    "preds_df = pd.DataFrame(preds)\n",
    "preds_df.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化预测结果\n",
    "tsmixer_preds = preds\n",
    "\n",
    "cols_to_plot = ['HUFL', 'HULL', 'MUFL', 'MULL']\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12,8))\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    col = cols_to_plot[i]\n",
    "    \n",
    "    ax.plot(test_data['date'][-96:], test_data[col][-96:])\n",
    "    ax.plot(test_data['date'][-96:], tsmixer_preds[col], label='TSMixer', ls='--', color='green')\n",
    "    \n",
    "    ax.legend(loc='best')\n",
    "    ax.set_xlabel('Time steps')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.set_title(col)\n",
    "    \n",
    "plt.tight_layout()\n",
    "fig.autofmt_xdate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
