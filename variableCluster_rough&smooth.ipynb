{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import random\n",
    "\n",
    "# 指定文件夹路径\n",
    "download_save_path = 'E:/Dataset/wind_shear/Data_Download'\n",
    "exception_save_path = '../Dataset/Exception_Data'\n",
    "\n",
    "# 获取文件夹下的所有文件名称\n",
    "download_folder_names = [item for item in os.listdir(download_save_path) if os.path.isdir(os.path.join(download_save_path, item))]\n",
    "exception_folder_names = [item for item in os.listdir(exception_save_path) if os.path.isdir(os.path.join(exception_save_path, item))]\n",
    "instruction_folder_names = [\"@Instructions\"]\n",
    "\n",
    "# 生成所有文件夹路径\n",
    "download_folder_paths = [os.path.join(download_save_path, item) for item in download_folder_names]\n",
    "exception_folder_paths = [os.path.join(exception_save_path, item) for item in exception_folder_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mechanism: 38\n",
      "power: 47\n",
      "control: 37\n",
      "external: 15\n",
      "recorder: 7\n",
      "unclassified: 19\n",
      "\n",
      "163 variables in total\n"
     ]
    }
   ],
   "source": [
    "# give the preset classification of variables\n",
    "group_names_list = [\"mechanism\", \"power\", \"control\", \"external\", \"recorder\", \"unclassified\"]\n",
    "group_lens_dict = {}\n",
    "\n",
    "var_group_mechanism = [\"AIL_1\", \"AIL_2\", \"FLAP\", \"ELEV_1\", \"ELEV_2\", \"RUDD\", \"SPL_1\", \"SPL_2\", \"SPLG\", \"SPLY\", \"ABRK\", \"BPGR_1\", \"BPGR_2\", \"BPYR_1\", \"BPYR_2\", \"MSQT_1\", \"MSQT_2\", \"NSQT\", \"BLV\", \"CALT\", \"PACK\", \"WOW\", \n",
    "                       \"AOA1\", \"AOA2\", \"GLS\", \"PTCH\", \"ROLL\", \n",
    "                       \"TH\", \"MH\", \"TAS\", \"CASM\", \"GS\", \"IVV\",\n",
    "                       \"VRTG\", \"LATG\", \"LONG\", \"FPAC\", \"CTAC\"]\n",
    "var_group_power = [\"N2_1\", \"N2_2\", \"N2_3\", \"N2_4\",\n",
    "                   \"ECYC_1\", \"ECYC_2\", \"ECYC_3\", \"ECYC_4\", \"EHRS_1\", \"EHRS_2\", \"EHRS_3\", \"EHRS_4\", \"VIB_1\", \"VIB_2\", \"VIB_3\", \"VIB_4\", \"FADS\", \"HYDG\", \"HYDY\",\n",
    "                   \"N1_1\", \"N1_2\", \"N1_3\", \"N1_4\", \"N1T\", \"FF_1\", \"FF_2\", \"FF_3\", \"FF_4\", \"FQTY_1\", \"FQTY_2\", \"FQTY_3\", \"FQTY_4\", \"OIP_1\", \"OIP_2\", \"OIP_3\", \"OIP_4\", \"OIT_1\", \"OIT_2\", \"OIT_3\", \"OIT_4\", \"OIPL\", \"EGT_1\", \"EGT_2\", \"EGT_3\", \"EGT_4\",\n",
    "                   \"LGDN\", \"LGUP\"]\n",
    "var_group_control = [\"CRSS\", \"HDGS\", \"A_T\", \"APFD\", \"DFGS\", \"FGC3\", \"PUSH\", \"PTRM\", \"TCAS\",\n",
    "                     \"ILSF\", \"RUDP\", \"CCPC\", \"CCPF\", \"CWPC\", \"CWPF\", \"PLA_1\", \"PLA_2\", \"PLA_3\", \"PLA_4\",\n",
    "                     \"SNAP\", \"TMODE\", \"EAI\", \"TAI\", \"WAI_1\", \"WAI_2\", \n",
    "                     \"APUF\", \"FADF\", \"FIRE_1\", \"FIRE_2\", \"FIRE_3\", \"FIRE_4\", \"GPWS\", \"MW\", \"POVT\", \"SHKR\", \"SMOK\", \"TOCW\"]\n",
    "var_group_external = [\"ALT\", \"ALTR\", \"WS\", \"WD\", \"PI\", \"PS\", \"PT\", \"SAT\", \"TAT\",\n",
    "                      \"DA\", \"TRK\", \"TRKM\", \"LOC\", \"LATP\", \"LONP\"]\n",
    "var_group_recorder = [\"DWPT\", \"PH\", \n",
    "                     \"ACMT\", \"FRMC\", \"GMT_HOUR\", \"GMT_MINUTE\", \"GMT_SEC\"]\n",
    "var_group_unclassified = [\"ATEN\", \"EVNT\", \"HF1\", \"HF2\", \"VHF1\", \"VHF2\", \"VHF3\", \"LMOD\", \"VMODE\", \"MACH\", \"MNS\", \"MRK\", \"N1C\", \"N1CO\", \"SMKB\", \"VAR_1107\", \"VAR_2670\", \"VAR_5107\", \"VAR_6670\"]\n",
    "\n",
    "var_groups_dict = {\"mechanism\": var_group_mechanism, \"power\": var_group_power, \"control\": var_group_control, \"external\": var_group_external, \"recorder\": var_group_recorder, \"unclassified\": var_group_unclassified}\n",
    "for group_name, var_group in var_groups_dict.items():\n",
    "    group_lens_dict[group_name] = len(var_group)\n",
    "    print(f\"{group_name}: {len(var_group)}\")\n",
    "print(f\"\\n{sum(group_lens_dict.values())} variables in total\")\n",
    "\n",
    "# 查找给定总序数对应的变量名称\n",
    "def find_var_name(idx, var_dict):\n",
    "    count = 0\n",
    "    group_lens_dict = {}\n",
    "    for group_name, var_group in var_dict.items():\n",
    "        group_lens_dict[group_name] = len(var_group)\n",
    "    for group_name, var_group in var_dict.items():\n",
    "        if count + group_lens_dict[group_name] > idx:\n",
    "            return group_name, var_group[idx - count]\n",
    "        else:\n",
    "            count += group_lens_dict[group_name]\n",
    "\n",
    "# 查找给定变量名称对应的总序数\n",
    "def find_var_idx(var_name, var_dict):\n",
    "    count = 0\n",
    "    for var_list in var_dict.values():\n",
    "        if var_name in var_list:\n",
    "            count += var_list.index(var_name)\n",
    "            return(count)\n",
    "        else:\n",
    "            count += len(var_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算粗糙度和光滑度矩阵\n",
    "\n",
    "from scipy.io import loadmat\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# 设置工作目录\n",
    "download_folder_name = 'Tail_652_1'\n",
    "\n",
    "# 设置结果存储目录\n",
    "cluster_save_path = '../result/variable_cluster/rough&smooth'\n",
    "if not os.path.exists(cluster_save_path):\n",
    "    os.mkdir(cluster_save_path)\n",
    "\n",
    "# 初始化pca结果存储array\n",
    "roughness_matrix_list = []\n",
    "smoothness_matrix_list = []\n",
    "\n",
    "for mat_name in os.listdir(os.path.join(download_save_path, download_folder_name)):\n",
    "    # 载入mat文件\n",
    "    mat = loadmat(os.path.join(download_save_path, download_folder_name, mat_name))\n",
    "    # 将mat文件整理成(163, )的array\n",
    "    wshr_data = mat[\"WSHR\"][0][0][0]\n",
    "    sampling_data_array = []\n",
    "    for var_list in var_groups_dict.values():\n",
    "        for var_name in var_list:\n",
    "            var_data, var_rate = mat[var_name][0][0][0], mat[var_name][0][0][1][0][0]\n",
    "            # 对每个变量按照rate进行下采样或过采样，对长为n+1的数据，抓取前n个全变量为输入，后n个有缺变量为输出\n",
    "            if var_rate == 1:\n",
    "                sampling_data = var_data\n",
    "            elif var_rate > 1: # 进行下采样\n",
    "                sampling_data = random.sample(var_data.tolist(), k=len(wshr_data))\n",
    "            else:\n",
    "                sampling_data = random.choices(var_data, k=len(wshr_data))\n",
    "            sampling_data_array.append(sampling_data)\n",
    "    summary_data_array = np.squeeze(np.array(sampling_data_array))\n",
    "    \n",
    "    # 计算粗糙度\n",
    "    roughness_matrix = summary_data_array[:, 1:] - summary_data_array[:, :-1]\n",
    "    roughness_matrix_list.append(roughness_matrix)\n",
    "    # 计算光滑程度\n",
    "    smoothness_matrix = roughness_matrix[:, 1:] - roughness_matrix[:, :-1]\n",
    "    smoothness_matrix_list.append(smoothness_matrix)\n",
    "\n",
    "    # print(summary_data_array.shape)\n",
    "    # print(roughness_matrix.shape)\n",
    "    # print(smoothness_matrix.shape)\n",
    "\n",
    "    # break\n",
    "\n",
    "# 存储粗糙度和光滑度list\n",
    "np.save(os.path.join(cluster_save_path, \"roughness_matrix_list.npy\"), roughness_matrix_list)\n",
    "np.save(os.path.join(cluster_save_path, \"smoothness_matrix_list.npy\"), smoothness_matrix_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行K-means聚类\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 初始化(粗糙度，光滑度)存储列表\n",
    "metrics_list = []\n",
    "\n",
    "# 将粗糙度和光滑度的均值tuple作为聚类metrics\n",
    "for i in range(len(roughness_matrix_list)):\n",
    "    metrics_list.append([np.mean(roughness_matrix_list[i]), np.mean(smoothness_matrix_list[i])])\n",
    "\n",
    "metrics_array = np.array(metrics_list)\n",
    "\n",
    "# 定义聚类方法\n",
    "def kmeans_clustering(data, K, max_iter=100):\n",
    "    # 初始化聚类中心\n",
    "    centers = []\n",
    "    centers.append(np.random.choice(data))\n",
    "    # centers.append(17500)\n",
    "    for _ in range(K-1):\n",
    "        distances = np.array([min([np.linalg.norm(x-c)**2 for c in centers]) for x in data])\n",
    "        prob = distances / distances.sum()\n",
    "        cumulative_prob = prob.cumsum()\n",
    "        r = np.random.rand()\n",
    "        for j, p in enumerate(cumulative_prob):\n",
    "            if r < p:\n",
    "                centers.append(data[j])\n",
    "                break\n",
    "\n",
    "    iIter = 0\n",
    "    for _ in range(max_iter):\n",
    "        # 计算每个样本与聚类中心的距离\n",
    "        distances = np.abs(data[:, np.newaxis] - centers)\n",
    "        # 将样本分配到最近的簇中\n",
    "        labels = np.argmin(distances, axis=1)\n",
    "        # 更新聚类中心\n",
    "        new_centers = np.array([np.mean(data[labels == k]) if np.sum(labels == k) != 0 else centers[k] for k in range(K)])\n",
    "        # 判断聚类中心是否变化\n",
    "        if np.allclose(centers, new_centers):\n",
    "            print(\"Iteration times = \", iIter)\n",
    "            break\n",
    "        centers = new_centers\n",
    "        iIter += 1\n",
    "    return labels, centers\n",
    "\n",
    "def kmeans_plus_plus_clustering(X, n_clusters):\n",
    "    X = np.array(X).reshape(-1, 1)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, init='k-means++')\n",
    "    kmeans.fit(X)\n",
    "    labels = kmeans.labels_\n",
    "    \n",
    "    return labels\n",
    "\n",
    "# 聚类\n",
    "K = 20\n",
    "# cluster_labels, cluster_centers = kmeans_clustering(np.array(dwpt_lists_flatten), K)\n",
    "cluster_labels = kmeans_plus_plus_clustering(metrics_array, K)\n",
    "np.save(os.path.join(cluster_save_path, \"cluster_labels.npy\"), cluster_labels)\n",
    "\n",
    "# # 打印聚类结果\n",
    "for k in range(K):\n",
    "    cluster_data = metrics_array[cluster_labels == k]\n",
    "    print(f\"Cluster std {k+1}: {np.std(cluster_data)}\")\n",
    "for k in range(K):\n",
    "    cluster_data = metrics_array[cluster_labels == k]\n",
    "    print(f\"Cluster size {k+1}: {cluster_data.shape}\")\n",
    "\n",
    "# # 打印聚类中心\n",
    "# print(\"Cluster centers:\", cluster_centers)\n",
    "\n",
    "# 可视化\n",
    "# selected_colors = ['r', 'g', 'b', 'c', 'm', 'y', 'k', 'pink', 'orange', 'purple']\n",
    "# colors = [selected_colors[i%len(selected_colors)] for i in cluster_labels]\n",
    "# plt.figure()\n",
    "# plt.scatter(cluster_labels, dwpt_lists_flatten, c=colors, s=10, cmap='viridis')\n",
    "# plt.xlabel(\"Cluster id\")\n",
    "# plt.ylabel(\"DWPT length\")\n",
    "# plt.title(\"DWPT clusters\")\n",
    "# plt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "# plt.savefig(os.path.join(dwpt_list_savepath, \"DWPT_clusters.png\"), bbox_inches='tight')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
