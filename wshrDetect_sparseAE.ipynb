{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import random\n",
    "\n",
    "# 指定文件夹路径\n",
    "download_save_path = 'E:/Dataset/wind_shear/Data_Download'\n",
    "exception_save_path = '../Dataset/Exception_Data'\n",
    "cluster_save_path = '../result/variable_cluster/rough&smooth'\n",
    "\n",
    "# 获取文件夹下的所有文件名称\n",
    "download_folder_names = [item for item in os.listdir(download_save_path) if os.path.isdir(os.path.join(download_save_path, item))]\n",
    "exception_folder_names = [item for item in os.listdir(exception_save_path) if os.path.isdir(os.path.join(exception_save_path, item))]\n",
    "instruction_folder_names = [\"@Instructions\"]\n",
    "\n",
    "# 生成所有文件夹路径\n",
    "download_folder_paths = [os.path.join(download_save_path, item) for item in download_folder_names]\n",
    "exception_folder_paths = [os.path.join(exception_save_path, item) for item in exception_folder_names]\n",
    "\n",
    "# 指定工作文件夹\n",
    "work_folder_path = exception_folder_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give the preset classification of variables\n",
    "group_names_list = [\"mechanism\", \"power\", \"control\", \"external\", \"recorder\", \"unclassified\"]\n",
    "group_lens_dict = {}\n",
    "\n",
    "var_group_mechanism = [\"AIL_1\", \"AIL_2\", \"FLAP\", \"ELEV_1\", \"ELEV_2\", \"RUDD\", \"SPL_1\", \"SPL_2\", \"SPLG\", \"SPLY\", \"ABRK\", \"BPGR_1\", \"BPGR_2\", \"BPYR_1\", \"BPYR_2\", \"MSQT_1\", \"MSQT_2\", \"NSQT\", \"BLV\", \"CALT\", \"PACK\", \"WOW\", \n",
    "                       \"AOA1\", \"AOA2\", \"GLS\", \"PTCH\", \"ROLL\", \n",
    "                       \"TH\", \"MH\", \"TAS\", \"CASM\", \"GS\", \"IVV\",\n",
    "                       \"VRTG\", \"LATG\", \"LONG\", \"FPAC\", \"CTAC\"]\n",
    "var_group_power = [\"N2_1\", \"N2_2\", \"N2_3\", \"N2_4\",\n",
    "                   \"ECYC_1\", \"ECYC_2\", \"ECYC_3\", \"ECYC_4\", \"EHRS_1\", \"EHRS_2\", \"EHRS_3\", \"EHRS_4\", \"VIB_1\", \"VIB_2\", \"VIB_3\", \"VIB_4\", \"FADS\", \"HYDG\", \"HYDY\",\n",
    "                   \"N1_1\", \"N1_2\", \"N1_3\", \"N1_4\", \"N1T\", \"FF_1\", \"FF_2\", \"FF_3\", \"FF_4\", \"FQTY_1\", \"FQTY_2\", \"FQTY_3\", \"FQTY_4\", \"OIP_1\", \"OIP_2\", \"OIP_3\", \"OIP_4\", \"OIT_1\", \"OIT_2\", \"OIT_3\", \"OIT_4\", \"OIPL\", \"EGT_1\", \"EGT_2\", \"EGT_3\", \"EGT_4\",\n",
    "                   \"LGDN\", \"LGUP\"]\n",
    "var_group_control = [\"CRSS\", \"HDGS\", \"A_T\", \"APFD\", \"DFGS\", \"FGC3\", \"PUSH\", \"PTRM\", \"TCAS\",\n",
    "                     \"ILSF\", \"RUDP\", \"CCPC\", \"CCPF\", \"CWPC\", \"CWPF\", \"PLA_1\", \"PLA_2\", \"PLA_3\", \"PLA_4\",\n",
    "                     \"SNAP\", \"TMODE\", \"EAI\", \"TAI\", \"WAI_1\", \"WAI_2\", \n",
    "                     \"APUF\", \"FADF\", \"FIRE_1\", \"FIRE_2\", \"FIRE_3\", \"FIRE_4\", \"GPWS\", \"MW\", \"POVT\", \"SHKR\", \"SMOK\", \"TOCW\"]\n",
    "var_group_external = [\"ALT\", \"ALTR\", \"WS\", \"WD\", \"PI\", \"PS\", \"PT\", \"SAT\", \"TAT\",\n",
    "                      \"DA\", \"TRK\", \"TRKM\", \"LOC\", \"LATP\", \"LONP\"]\n",
    "var_group_recorder = [\"DWPT\", \"PH\", \n",
    "                     \"ACMT\", \"FRMC\", \"GMT_HOUR\", \"GMT_MINUTE\", \"GMT_SEC\"]\n",
    "var_group_unclassified = [\"ATEN\", \"EVNT\", \"HF1\", \"HF2\", \"VHF1\", \"VHF2\", \"VHF3\", \"LMOD\", \"VMODE\", \"MACH\", \"MNS\", \"MRK\", \"N1C\", \"N1CO\", \"SMKB\", \"VAR_1107\", \"VAR_2670\", \"VAR_5107\", \"VAR_6670\"]\n",
    "\n",
    "var_groups_dict = {\"mechanism\": var_group_mechanism, \"power\": var_group_power, \"control\": var_group_control, \"external\": var_group_external, \"recorder\": var_group_recorder, \"unclassified\": var_group_unclassified}\n",
    "for group_name, var_group in var_groups_dict.items():\n",
    "    group_lens_dict[group_name] = len(var_group)\n",
    "    print(f\"{group_name}: {len(var_group)}\")\n",
    "print(f\"\\n{sum(group_lens_dict.values())} variables in total\")\n",
    "\n",
    "# 查找给定总序数对应的变量名称\n",
    "def find_var_name(idx, var_dict):\n",
    "    count = 0\n",
    "    group_lens_dict = {}\n",
    "    for group_name, var_group in var_dict.items():\n",
    "        group_lens_dict[group_name] = len(var_group)\n",
    "    for group_name, var_group in var_dict.items():\n",
    "        if count + group_lens_dict[group_name] > idx:\n",
    "            return group_name, var_group[idx - count]\n",
    "        else:\n",
    "            count += group_lens_dict[group_name]\n",
    "\n",
    "# 查找给定变量名称对应的总序数\n",
    "def find_var_idx(var_name, var_dict):\n",
    "    count = 0\n",
    "    for var_list in var_dict.values():\n",
    "        if var_name in var_list:\n",
    "            count += var_list.index(var_name)\n",
    "            return(count)\n",
    "        else:\n",
    "            count += len(var_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集构建\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# 读取采样的npy文件\n",
    "sample_save_path = '../Dataset/Samples/sampling_data_array.npy'\n",
    "# os.chmod(sample_save_path, stat.S_IRWXU)\n",
    "sampling_data_array = np.load(sample_save_path)\n",
    "\n",
    "def datasetConstruct(segments, test_idx):\n",
    "    if test_idx == 0:\n",
    "        train_dataset = np.squeeze(np.array(np.concatenate(segments[-(k-1-test_idx):], axis=1)))\n",
    "    elif test_idx == k-1:\n",
    "        train_dataset = np.squeeze(np.array(np.concatenate(segments[:test_idx], axis=1)))\n",
    "    else:\n",
    "        f_seg_array, b_seg_array = np.squeeze(np.array(np.concatenate(segments[:test_idx], axis=1))), np.squeeze(np.array(np.concatenate(segments[-(k-1-test_idx):], axis=1)))\n",
    "        train_dataset = np.concatenate((f_seg_array, b_seg_array), axis=1)\n",
    "    test_dataset = segments[test_idx]\n",
    "    # train_X, train_Y = train_dataset[1:], train_dataset[0]\n",
    "    # test_X, test_Y = test_dataset[1:], test_dataset[0]\n",
    "    return np.squeeze(train_dataset), np.squeeze(test_dataset)\n",
    "\n",
    "# train_dataset, test_dataset = datasetConstruct(segments, 0)\n",
    "# print(train_dataset.shape, test_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32md:\\Documents\\Learn\\PhD_year1\\项目\\wind\\code\\wshrDetect_sparseAE.ipynb Cell 4\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/Learn/PhD_year1/%E9%A1%B9%E7%9B%AE/wind/code/wshrDetect_sparseAE.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptim\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39moptim\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/Learn/PhD_year1/%E9%A1%B9%E7%9B%AE/wind/code/wshrDetect_sparseAE.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Documents/Learn/PhD_year1/%E9%A1%B9%E7%9B%AE/wind/code/wshrDetect_sparseAE.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m StandardScaler\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/Learn/PhD_year1/%E9%A1%B9%E7%9B%AE/wind/code/wshrDetect_sparseAE.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# 定义稀疏自编码器模型\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents/Learn/PhD_year1/%E9%A1%B9%E7%9B%AE/wind/code/wshrDetect_sparseAE.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mSparseAutoencoder\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 定义稀疏自编码器模型\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(SparseAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, encoding_dim),\n",
    "            nn.Sigmoid()  # 用Sigmoid激活函数产生稀疏性\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# 准备输入数据\n",
    "input_dim = 163  # 输入数据维度\n",
    "encoding_dim = 1  # 编码后的维度\n",
    "# 定义每个batch的大小\n",
    "batch_size = 128\n",
    "\n",
    "def sparseAE(train_dataset, test_dataset, test_idx, response_var_name):\n",
    "    # 查找预测变量编号\n",
    "    if response_var_name == 'WSHR':\n",
    "        response_var_idx = 0\n",
    "    else:\n",
    "        response_var_idx = find_var_idx(response_var_name, var_groups_dict) + 1      \n",
    "    \n",
    "    # 创建权重记录列表\n",
    "    weights_record_list = []\n",
    "\n",
    "    # 计算总的batch数量\n",
    "    num_batches = train_dataset.shape[1] // batch_size\n",
    "\n",
    "    # 随机打乱数据集的索引\n",
    "    indices = np.random.permutation(train_dataset.shape[1])\n",
    "\n",
    "    train_X, train_Y = (np.delete(train_dataset, [0, response_var_idx], axis=0)).T, train_dataset[response_var_idx]\n",
    "    test_X, test_Y = (np.delete(test_dataset, [0, response_var_idx], axis=0)).T, test_dataset[response_var_idx]\n",
    "\n",
    "    # 特征标准化处理\n",
    "    scaler = StandardScaler()\n",
    "    train_X_scaled = scaler.fit_transform(train_X)\n",
    "    test_X_scaled = scaler.transform(test_X)\n",
    "\n",
    "    # 创建稀疏自编码器模型\n",
    "    model = SparseAutoencoder(input_dim, encoding_dim)\n",
    "\n",
    "    # 定义损失函数和优化器\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # 按照每个batch逐批训练\n",
    "    for i in range(num_batches):\n",
    "        # 获取当前batch的索引\n",
    "        batch_indices = indices[i * batch_size: (i + 1) * batch_size]\n",
    "\n",
    "        # 获取当前batch的数据\n",
    "        batch_X = train_X_scaled[batch_indices]\n",
    "        batch_Y = train_Y[batch_indices]\n",
    "\n",
    "        # 逐批训练模型\n",
    "        # 前向传播\n",
    "        output = model(batch_X)\n",
    "        loss = criterion(output, batch_Y)\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 调用回调函数\n",
    "        # weights = mlp.coefs_\n",
    "        # weights_record_list.append(weights)\n",
    "\n",
    "    # 使用稀疏自编码器进行编码\n",
    "    encoded_data = model.encoder(test_X)\n",
    "    print(encoded_data, test_Y)\n",
    "\n",
    "# k-fold分割训练集和测试集\n",
    "k = 10\n",
    "segments = np.array_split(sampling_data_array, k, axis=1)\n",
    "train_dataset, test_dataset = datasetConstruct(segments, 0)\n",
    "sparseAE(train_dataset, test_dataset, 0, response_var_name='WSHR')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
