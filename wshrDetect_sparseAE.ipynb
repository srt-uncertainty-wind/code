{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import random\n",
    "\n",
    "# 指定文件夹路径\n",
    "download_save_path = 'E:/Dataset/wind_shear/Data_Download'\n",
    "exception_save_path = '../Dataset/Exception_Data'\n",
    "cluster_save_path = '../result/variable_cluster/rough&smooth'\n",
    "\n",
    "# 获取文件夹下的所有文件名称\n",
    "download_folder_names = [item for item in os.listdir(download_save_path) if os.path.isdir(os.path.join(download_save_path, item))]\n",
    "exception_folder_names = [item for item in os.listdir(exception_save_path) if os.path.isdir(os.path.join(exception_save_path, item))]\n",
    "instruction_folder_names = [\"@Instructions\"]\n",
    "\n",
    "# 生成所有文件夹路径\n",
    "download_folder_paths = [os.path.join(download_save_path, item) for item in download_folder_names]\n",
    "exception_folder_paths = [os.path.join(exception_save_path, item) for item in exception_folder_names]\n",
    "\n",
    "# 指定工作文件夹\n",
    "work_folder_path = exception_folder_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mechanism: 38\n",
      "power: 47\n",
      "control: 37\n",
      "external: 15\n",
      "recorder: 7\n",
      "unclassified: 19\n",
      "\n",
      "163 variables in total\n"
     ]
    }
   ],
   "source": [
    "# give the preset classification of variables\n",
    "group_names_list = [\"mechanism\", \"power\", \"control\", \"external\", \"recorder\", \"unclassified\"]\n",
    "group_lens_dict = {}\n",
    "\n",
    "var_group_mechanism = [\"AIL_1\", \"AIL_2\", \"FLAP\", \"ELEV_1\", \"ELEV_2\", \"RUDD\", \"SPL_1\", \"SPL_2\", \"SPLG\", \"SPLY\", \"ABRK\", \"BPGR_1\", \"BPGR_2\", \"BPYR_1\", \"BPYR_2\", \"MSQT_1\", \"MSQT_2\", \"NSQT\", \"BLV\", \"CALT\", \"PACK\", \"WOW\", \n",
    "                       \"AOA1\", \"AOA2\", \"GLS\", \"PTCH\", \"ROLL\", \n",
    "                       \"TH\", \"MH\", \"TAS\", \"CASM\", \"GS\", \"IVV\",\n",
    "                       \"VRTG\", \"LATG\", \"LONG\", \"FPAC\", \"CTAC\"]\n",
    "var_group_power = [\"N2_1\", \"N2_2\", \"N2_3\", \"N2_4\",\n",
    "                   \"ECYC_1\", \"ECYC_2\", \"ECYC_3\", \"ECYC_4\", \"EHRS_1\", \"EHRS_2\", \"EHRS_3\", \"EHRS_4\", \"VIB_1\", \"VIB_2\", \"VIB_3\", \"VIB_4\", \"FADS\", \"HYDG\", \"HYDY\",\n",
    "                   \"N1_1\", \"N1_2\", \"N1_3\", \"N1_4\", \"N1T\", \"FF_1\", \"FF_2\", \"FF_3\", \"FF_4\", \"FQTY_1\", \"FQTY_2\", \"FQTY_3\", \"FQTY_4\", \"OIP_1\", \"OIP_2\", \"OIP_3\", \"OIP_4\", \"OIT_1\", \"OIT_2\", \"OIT_3\", \"OIT_4\", \"OIPL\", \"EGT_1\", \"EGT_2\", \"EGT_3\", \"EGT_4\",\n",
    "                   \"LGDN\", \"LGUP\"]\n",
    "var_group_control = [\"CRSS\", \"HDGS\", \"A_T\", \"APFD\", \"DFGS\", \"FGC3\", \"PUSH\", \"PTRM\", \"TCAS\",\n",
    "                     \"ILSF\", \"RUDP\", \"CCPC\", \"CCPF\", \"CWPC\", \"CWPF\", \"PLA_1\", \"PLA_2\", \"PLA_3\", \"PLA_4\",\n",
    "                     \"SNAP\", \"TMODE\", \"EAI\", \"TAI\", \"WAI_1\", \"WAI_2\", \n",
    "                     \"APUF\", \"FADF\", \"FIRE_1\", \"FIRE_2\", \"FIRE_3\", \"FIRE_4\", \"GPWS\", \"MW\", \"POVT\", \"SHKR\", \"SMOK\", \"TOCW\"]\n",
    "var_group_external = [\"ALT\", \"ALTR\", \"WS\", \"WD\", \"PI\", \"PS\", \"PT\", \"SAT\", \"TAT\",\n",
    "                      \"DA\", \"TRK\", \"TRKM\", \"LOC\", \"LATP\", \"LONP\"]\n",
    "var_group_recorder = [\"DWPT\", \"PH\", \n",
    "                     \"ACMT\", \"FRMC\", \"GMT_HOUR\", \"GMT_MINUTE\", \"GMT_SEC\"]\n",
    "var_group_unclassified = [\"ATEN\", \"EVNT\", \"HF1\", \"HF2\", \"VHF1\", \"VHF2\", \"VHF3\", \"LMOD\", \"VMODE\", \"MACH\", \"MNS\", \"MRK\", \"N1C\", \"N1CO\", \"SMKB\", \"VAR_1107\", \"VAR_2670\", \"VAR_5107\", \"VAR_6670\"]\n",
    "\n",
    "var_groups_dict = {\"mechanism\": var_group_mechanism, \"power\": var_group_power, \"control\": var_group_control, \"external\": var_group_external, \"recorder\": var_group_recorder, \"unclassified\": var_group_unclassified}\n",
    "for group_name, var_group in var_groups_dict.items():\n",
    "    group_lens_dict[group_name] = len(var_group)\n",
    "    print(f\"{group_name}: {len(var_group)}\")\n",
    "print(f\"\\n{sum(group_lens_dict.values())} variables in total\")\n",
    "\n",
    "# 查找给定总序数对应的变量名称\n",
    "def find_var_name(idx, var_dict):\n",
    "    count = 0\n",
    "    group_lens_dict = {}\n",
    "    for group_name, var_group in var_dict.items():\n",
    "        group_lens_dict[group_name] = len(var_group)\n",
    "    for group_name, var_group in var_dict.items():\n",
    "        if count + group_lens_dict[group_name] > idx:\n",
    "            return group_name, var_group[idx - count]\n",
    "        else:\n",
    "            count += group_lens_dict[group_name]\n",
    "\n",
    "# 查找给定变量名称对应的总序数\n",
    "def find_var_idx(var_name, var_dict):\n",
    "    count = 0\n",
    "    for var_list in var_dict.values():\n",
    "        if var_name in var_list:\n",
    "            count += var_list.index(var_name)\n",
    "            return(count)\n",
    "        else:\n",
    "            count += len(var_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集构建\n",
    "def dataSampling(var_data, var_rate, wshr_data):\n",
    "    # 对每个变量按照rate进行下采样或过采样，对长为n+1的数据，抓取前n个全变量为输入，后n个有缺变量为输出\n",
    "    if var_rate == 1:\n",
    "        sampling_data = var_data\n",
    "    else: # 进行重采样\n",
    "        sampling_data = [var_data[i] for i in np.linspace(0, len(var_data)- 1, len(wshr_data), dtype=int)]\n",
    "    # 将采样数据进行min_max归一化\n",
    "    if (np.max(sampling_data) - np.min(sampling_data)) > 1e-5:\n",
    "        sampling_data = (sampling_data - np.min(sampling_data)) / (np.max(sampling_data) - np.min(sampling_data))\n",
    "    elif np.mean(sampling_data) > 1e-5:\n",
    "        sampling_data = sampling_data / np.mean(sampling_data)\n",
    "    else:\n",
    "        sampling_data = sampling_data\n",
    "    return sampling_data\n",
    "\n",
    "def dataConstruct(work_folder_path, work_mat_name):\n",
    "    # 读取工作mat\n",
    "    work_mat = loadmat(os.path.join(work_folder_path, work_mat_name))\n",
    "\n",
    "    # 初始化采样数据list\n",
    "    sampling_data_list = []\n",
    "    wshr_data = work_mat[\"WSHR\"][0][0][0]\n",
    "\n",
    "    for var_list in var_groups_dict.values():\n",
    "        for var_name in var_list:\n",
    "            var_data, var_rate = work_mat[var_name][0][0][0], work_mat[var_name][0][0][1][0][0]\n",
    "            sampling_data = dataSampling(var_data, var_rate, wshr_data)\n",
    "            # print(np.max(sampling_data), np.min(sampling_data))\n",
    "            sampling_data_list.append(sampling_data)\n",
    "    sampling_data_list.append(wshr_data)\n",
    "    sampling_data_list = np.squeeze(np.array(sampling_data_list))\n",
    "\n",
    "    # 生成解释变量X和分类变量Y\n",
    "    dataset = np.squeeze(np.array(sampling_data_list)).T\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(163, 7852) (7852, 1) (163, 8656) (8656, 1)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 96\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# autoEncoder.to(device) \u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m---> 96\u001b[0m     \u001b[43mautoEncoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 注:将模型放到GPU上,因此后续传入的数据必须也在GPU上\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Loss = nn.BCELoss()\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Optimizer = optim.Adam(autoEncoder.parameters(), lr=0.001)\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    161\u001b[0m \n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m#     print('Epoch: {}, Loss: {:.4f}, Time: {:.2f}'.format(epoch + 1, loss, time.time() - time_epoch_start))\u001b[39;00m\n",
      "File \u001b[1;32md:\\coding\\Anaconda\\Software\\envs\\pytorch38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\coding\\Anaconda\\Software\\envs\\pytorch38\\lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32md:\\coding\\Anaconda\\Software\\envs\\pytorch38\\lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32md:\\coding\\Anaconda\\Software\\envs\\pytorch38\\lib\\site-packages\\torch\\nn\\modules\\module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32md:\\coding\\Anaconda\\Software\\envs\\pytorch38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*-coding:utf-8-*-\n",
    "\n",
    "''' Revised from reference/SparseAutoEncoder.py\n",
    "https://github.com/LitoNeo/pytorch-AutoEncoders/blob/master/src/SparseAutoEncoder.py\n",
    "'''\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional\n",
    "import torch.optim as optim\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# 将默认的计算设备设置为CPU\n",
    "# print(torch.cuda.current_device())\n",
    "# torch.cuda.set_device(torch.device('cpu'))\n",
    "\n",
    "batch_size = 100\n",
    "num_epochs = 50\n",
    "in_dim = 163\n",
    "hidden_size = 50\n",
    "expect_tho = 0.05\n",
    "\n",
    "def KL_devergence(p, q):\n",
    "    \"\"\"\n",
    "    Calculate the KL-divergence of (p,q)\n",
    "    :param p:\n",
    "    :param q:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    q = torch.nn.functional.softmax(q, dim=0)\n",
    "    q = torch.sum(q, dim=0)/batch_size  # dim:缩减的维度,q的第一维是batch维,即大小为batch_size大小,此处是将第j个神经元在batch_size个输入下所有的输出取平均\n",
    "    s1 = torch.sum(p*torch.log(p/q))\n",
    "    s2 = torch.sum((1-p)*torch.log((1-p)/(1-q)))\n",
    "    return s1+s2\n",
    "\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, in_dim=163, hidden_size=50, out_dim=163):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_features=in_dim, out_features=hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(in_features=hidden_size, out_features=out_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoder_out = self.encoder(x)\n",
    "        decoder_out = self.decoder(encoder_out)\n",
    "        return encoder_out, decoder_out\n",
    "\n",
    "\n",
    "# 构建训练和测试数据集\n",
    "train_dataset = dataConstruct(exception_folder_paths[3], os.listdir(exception_folder_paths[3])[2])\n",
    "test_dataset = dataConstruct(exception_folder_paths[3], os.listdir(exception_folder_paths[3])[8])\n",
    "\n",
    "# 计算总的batch数量\n",
    "num_batches = train_dataset.shape[1] // batch_size\n",
    "\n",
    "# 随机打乱数据集的索引\n",
    "indices = np.random.permutation(train_dataset.shape[1])\n",
    "\n",
    "train_X, train_Y = (np.delete(train_dataset, [train_dataset.shape[1]-1], axis=1)).T, train_dataset[:, train_dataset.shape[1]-1].reshape(-1,1)\n",
    "test_X, test_Y = (np.delete(test_dataset, [test_dataset.shape[1]-1], axis=1)).T, test_dataset[:, test_dataset.shape[1]-1].reshape(-1,1)\n",
    "print(train_X.shape, train_Y.shape, test_X.shape, test_Y.shape)\n",
    "# print(np.max(train_X), np.min(train_X))\n",
    "\n",
    "# # 特征标准化处理\n",
    "# scaler = StandardScaler()\n",
    "# train_X_scaled = scaler.fit_transform(train_X)\n",
    "# test_X_scaled = scaler.transform(test_X)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# 构建稀疏自编码器模型\n",
    "autoEncoder = AutoEncoder(in_dim=in_dim, hidden_size=hidden_size, out_dim=in_dim)\n",
    "# autoEncoder.to(device) \n",
    "if torch.cuda.is_available():\n",
    "    autoEncoder.to(device)  # 注:将模型放到GPU上,因此后续传入的数据必须也在GPU上\n",
    "\n",
    "Loss = nn.BCELoss()\n",
    "Optimizer = optim.Adam(autoEncoder.parameters(), lr=0.001)\n",
    "\n",
    "# 定义期望平均激活值和KL散度的权重\n",
    "tho_tensor = torch.FloatTensor([expect_tho for _ in range(hidden_size)])\n",
    "tho_tensor = tho_tensor.to(device)\n",
    "# if torch.cuda.is_available():\n",
    "#     tho_tensor = tho_tensor.to(device)\n",
    "_beta = 3\n",
    "\n",
    "# def kl_1(p, q):\n",
    "#     p = torch.nn.functional.softmax(p, dim=-1)\n",
    "#     _kl = torch.sum(p*(torch.log_softmax(p,dim=-1)) - torch.nn.functional.log_softmax(q, dim=-1),1)\n",
    "#     return torch.mean(_kl)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    time_epoch_start = time.time()\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        # 获取当前batch的索引\n",
    "        batch_indices = indices[i * batch_size: (i + 1) * batch_size]\n",
    "\n",
    "        # 获取当前batch的数据\n",
    "        batch_X = train_X[:, batch_indices]\n",
    "        batch_Y = train_Y[batch_indices]\n",
    "        # print(batch_X.shape, batch_Y.shape)\n",
    "\n",
    "        # 转移到gpu\n",
    "        batch_X = torch.from_numpy(batch_X).float().T.to(device)\n",
    "        batch_Y = torch.from_numpy(batch_Y).float().to(device)\n",
    "        # if torch.cuda.is_available():\n",
    "        #     batch_X = torch.from_numpy(batch_X).float().T.to(device)\n",
    "        #     batch_Y = torch.from_numpy(batch_Y).float().to(device)\n",
    "        \n",
    "        # 开始训练\n",
    "        encoder_out, decoder_out = autoEncoder(batch_X)\n",
    "        loss = Loss(decoder_out, batch_X)\n",
    "\n",
    "        # 计算并增加KL散度到loss\n",
    "        _kl = KL_devergence(tho_tensor, encoder_out)\n",
    "        loss += _beta * _kl\n",
    "\n",
    "        Optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        Optimizer.step()\n",
    "\n",
    "        print('Epoch: {}, Loss: {:.4f}, Time: {:.2f}'.format(epoch + 1, loss, time.time() - time_epoch_start))\n",
    "\n",
    "    # for batch_index, (train_data, train_label) in enumerate(train_loader):\n",
    "    #     if torch.cuda.is_available():\n",
    "    #         train_data = train_data.cuda()\n",
    "    #         train_label = train_label.cuda()\n",
    "    #     input_data = train_data.view(train_data.size(0), -1)\n",
    "    #     encoder_out, decoder_out = autoEncoder(input_data)\n",
    "    #     loss = Loss(decoder_out, input_data)\n",
    "\n",
    "    #     # 计算并增加KL散度到loss\n",
    "    #     _kl = KL_devergence(tho_tensor, encoder_out)\n",
    "    #     loss += _beta * _kl\n",
    "\n",
    "    #     Optimizer.zero_grad()\n",
    "    #     loss.backward()\n",
    "    #     Optimizer.step()\n",
    "\n",
    "    #     print('Epoch: {}, Loss: {:.4f}, Time: {:.2f}'.format(epoch + 1, loss, time.time() - time_epoch_start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 4060 Laptop GPU'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
